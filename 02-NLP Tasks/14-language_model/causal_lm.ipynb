{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 因果语言模型训练实例"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step1 导入相关包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\.conda\\envs\\py310torch\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, DataCollatorForLanguageModeling, TrainingArguments, Trainer, BloomForCausalLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step2 加载数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = Dataset.load_from_disk(\"./wiki_cn_filtered/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['source', 'completion'],\n",
       "    num_rows: 10000\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'source': 'wikipedia.zh2307',\n",
       " 'completion': \"西安交通大学博物馆（Xi'an Jiaotong University Museum）是一座位于西安交通大学的博物馆，馆长是锺明善。\\n历史\\n2004年9月20日开始筹建，2013年4月8日正式建成开馆，位于西安交通大学兴庆校区陕西省西安市咸宁西路28号。建筑面积6,800平米，展厅面积4,500平米，馆藏文物4,900余件。包括历代艺术文物馆、碑石书法馆、西部农民画馆、邢良坤陶瓷艺术馆、陕西秦腔博物馆和书画展厅共五馆一厅。\\n营业时间\\n* 周一至周六：上午九点至十二点，下午一点至五点\\n* 周日闭馆\"}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step3 数据集处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"Langboat/bloom-389m-zh\")\n",
    "\n",
    "def process_func(examples):\n",
    "    contents = [e + tokenizer.eos_token for e in examples[\"completion\"]]\n",
    "    return tokenizer(contents, max_length=384, truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'attention_mask'],\n",
       "    num_rows: 10000\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_ds = ds.map(process_func, batched=True, remove_columns=ds.column_names)\n",
    "tokenized_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "dl = DataLoader(tokenized_ds, batch_size=2, collate_fn=DataCollatorForLanguageModeling(tokenizer, mlm=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0,\n",
       " {'input_ids': tensor([[    3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
       "              3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
       "              3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
       "              3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
       "              3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
       "              3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
       "              3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
       "              3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
       "              3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
       "              3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
       "              3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
       "              3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
       "              3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
       "              3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
       "              3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
       "              3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
       "              3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
       "              3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
       "              3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
       "              3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
       "              3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
       "              3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
       "              3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
       "              3,     3,     3,     3,     3,     3,     3,     3, 13110, 34800,\n",
       "          13535,   916, 33156,    10,   256,   576,   387,   479,   681,  5453,\n",
       "          10955,   915, 24124,  5317, 13110,  6573, 20757, 13535,   355,  5358,\n",
       "           1490,   583, 28056,  1407,  3855,   671,  6113,   189,  6732,  4302,\n",
       "           9488,  3434,  6900,  1322,   355, 37336,  9825,  4608, 13461,  1359,\n",
       "           5358,   355,  5317, 13110, 34800,  4433,  7189, 25722, 29747, 13110,\n",
       "           1498, 12047,  6347, 23563,  2139,  2066,   420, 29288,    25,    15,\n",
       "           7635, 39288,   355,  1484,  5835,  6272,    23,    15,  4180, 39288,\n",
       "            355,  5358,  4516, 11621,    23,    15, 10641,  4887,  1712,   420,\n",
       "           2450, 31163,  8085, 11621,  5358,   553,  9888,  2731, 21335,  5358,\n",
       "            553,  9876, 14011,  4434,  5358,   553, 21484,  4514, 17170, 25871,\n",
       "           8085,  5358,   553, 17489,  6945, 11097, 13535,   641, 33623,  1484,\n",
       "           5835,  1689,  2063,  5358,   569,  5835,   671, 16225,  3422,   189,\n",
       "             13, 23158, 27894, 33227,  1022, 11396,  3347,  1813,  1504,  6566,\n",
       "           1813,   355,  9155,  8633,  1504,  2063,  1813,   189,    13, 23158,\n",
       "            813,  7817,  5358,     2],\n",
       "         [  586, 11835,   739,   355,  8417,  2300,   916, 16892,  5077,   580,\n",
       "          15549,  1434,   996,   307,   387,   355,  1997,  8236,   775,  8417,\n",
       "           2152,  6496, 12176,  3728,  7692,  1503,  1528,  1014, 17887, 18001,\n",
       "           3250,   355,  6250,  3896,  2670, 20882, 16080, 14005,  3993,  1503,\n",
       "          12295,  8930,  3250,   420,  4208,  4326,  5367,  8417,  2152,  4632,\n",
       "          37591,   355,  1379,  8417,  2300, 32824,  3250,  9015, 18945, 16714,\n",
       "           5908, 13551, 30330, 23756,  2152, 15976,   657,  5923,  8417,   586,\n",
       "          16080,  1528,  8941,  5917, 14035,  5895, 14092,  4353, 29445,   355,\n",
       "           8790, 21595,  1450, 15911, 31116, 10345, 29940, 10874,  1125,  4829,\n",
       "          16080,  7093, 22939,   737,   262,   387,    72,   272,   831,   455,\n",
       "             72,   915,  8860, 20725,  1934,  1084,  5478,   420,  4415,  8417,\n",
       "          26263, 12726,   553, 10875, 34820,   355,  1266,  5498,   586, 14907,\n",
       "          32795, 11835,   904, 19934,  1528, 19531, 20517,  1349, 19472, 28879,\n",
       "            671,  8417, 26263, 17020,  5963, 22388, 11900, 12669, 13240,  1042,\n",
       "           9783,   355,  7242,   714,  1806,   775,  6500,   355, 11526, 10185,\n",
       "           1293,  1665, 15984,  7092,  1617,  8417,  2300,   420, 19972, 25622,\n",
       "          10875, 17500, 26523,  2391,  8417,  2300,   355,  6751,  2836, 13539,\n",
       "           8247,   373, 30201,  5498,   420,  8417,  2300,   586,  8523, 19358,\n",
       "           1298, 12176, 30939, 10739,   964,  4318, 10875,   420, 11900, 16080,\n",
       "            904,  9783,   355, 22464,  9658,   355,  8417,  2300, 13561,  2054,\n",
       "           4983,  4829, 30800,  7262,   420, 11394,  8417, 35143, 11937, 15682,\n",
       "           8417,  2300,  7283, 10875,   355,  1016,  4179,  5039, 14027, 26215,\n",
       "          26835,   671, 15095,   189,  1165, 15095, 11900,  6184,  1125,  3244,\n",
       "           3687,   622,  8785,  1121,   891, 13765,   671, 10199,   189,    13,\n",
       "            210,  6940,  3728,  9552,  6082,  8417,  2300,   916,  4375,   714,\n",
       "           3679,  1806, 10567,   915,   189,    13,   210, 16131, 11835,  8417,\n",
       "           2300,   189,    13, 24075,  3728,  8417,  2300,   916,  4829,  3687,\n",
       "           9000, 27689,  8417,  2300,  1300, 11243,  2062, 28431, 27689, 11835,\n",
       "           8417,  2300, 13224,  4829,  3687, 15964,   915,   189,    13, 27340,\n",
       "          11835,  8417,  2300,   189,    13, 27340,  3982,  3728,  8417,  2300,\n",
       "            916,  4375,  2450,  3272,  1234, 19083,   553,  3512,  3121,  1728,\n",
       "            641,  3092,  2113,  7843,   915,   189,    13,   210, 15402,  8417,\n",
       "           2300,   189,    13, 10057,   108, 12693, 14624, 29379,  4719,  6533,\n",
       "            739,   916, 16148, 10981, 21350,  9067,  1203,  8931,  1258, 11835,\n",
       "           4719,  6533,   739, 20393,   189,    13,   210, 19546,  1517, 11835,\n",
       "           8417,  2300,   189,    13,   210, 23928,   168,   117,   245,  6279,\n",
       "            114,   240,   170,   100,   124,   168,   117,   228,  6279,   100,\n",
       "            124,   168,   117,   228,   171,   238,   224, 41356,   236, 24175,\n",
       "          11082, 10981, 21350,  9067]]), 'attention_mask': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]), 'labels': tensor([[ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100, 13110, 34800,\n",
       "          13535,   916, 33156,    10,   256,   576,   387,   479,   681,  5453,\n",
       "          10955,   915, 24124,  5317, 13110,  6573, 20757, 13535,   355,  5358,\n",
       "           1490,   583, 28056,  1407,  3855,   671,  6113,   189,  6732,  4302,\n",
       "           9488,  3434,  6900,  1322,   355, 37336,  9825,  4608, 13461,  1359,\n",
       "           5358,   355,  5317, 13110, 34800,  4433,  7189, 25722, 29747, 13110,\n",
       "           1498, 12047,  6347, 23563,  2139,  2066,   420, 29288,    25,    15,\n",
       "           7635, 39288,   355,  1484,  5835,  6272,    23,    15,  4180, 39288,\n",
       "            355,  5358,  4516, 11621,    23,    15, 10641,  4887,  1712,   420,\n",
       "           2450, 31163,  8085, 11621,  5358,   553,  9888,  2731, 21335,  5358,\n",
       "            553,  9876, 14011,  4434,  5358,   553, 21484,  4514, 17170, 25871,\n",
       "           8085,  5358,   553, 17489,  6945, 11097, 13535,   641, 33623,  1484,\n",
       "           5835,  1689,  2063,  5358,   569,  5835,   671, 16225,  3422,   189,\n",
       "             13, 23158, 27894, 33227,  1022, 11396,  3347,  1813,  1504,  6566,\n",
       "           1813,   355,  9155,  8633,  1504,  2063,  1813,   189,    13, 23158,\n",
       "            813,  7817,  5358,     2],\n",
       "         [  586, 11835,   739,   355,  8417,  2300,   916, 16892,  5077,   580,\n",
       "          15549,  1434,   996,   307,   387,   355,  1997,  8236,   775,  8417,\n",
       "           2152,  6496, 12176,  3728,  7692,  1503,  1528,  1014, 17887, 18001,\n",
       "           3250,   355,  6250,  3896,  2670, 20882, 16080, 14005,  3993,  1503,\n",
       "          12295,  8930,  3250,   420,  4208,  4326,  5367,  8417,  2152,  4632,\n",
       "          37591,   355,  1379,  8417,  2300, 32824,  3250,  9015, 18945, 16714,\n",
       "           5908, 13551, 30330, 23756,  2152, 15976,   657,  5923,  8417,   586,\n",
       "          16080,  1528,  8941,  5917, 14035,  5895, 14092,  4353, 29445,   355,\n",
       "           8790, 21595,  1450, 15911, 31116, 10345, 29940, 10874,  1125,  4829,\n",
       "          16080,  7093, 22939,   737,   262,   387,    72,   272,   831,   455,\n",
       "             72,   915,  8860, 20725,  1934,  1084,  5478,   420,  4415,  8417,\n",
       "          26263, 12726,   553, 10875, 34820,   355,  1266,  5498,   586, 14907,\n",
       "          32795, 11835,   904, 19934,  1528, 19531, 20517,  1349, 19472, 28879,\n",
       "            671,  8417, 26263, 17020,  5963, 22388, 11900, 12669, 13240,  1042,\n",
       "           9783,   355,  7242,   714,  1806,   775,  6500,   355, 11526, 10185,\n",
       "           1293,  1665, 15984,  7092,  1617,  8417,  2300,   420, 19972, 25622,\n",
       "          10875, 17500, 26523,  2391,  8417,  2300,   355,  6751,  2836, 13539,\n",
       "           8247,   373, 30201,  5498,   420,  8417,  2300,   586,  8523, 19358,\n",
       "           1298, 12176, 30939, 10739,   964,  4318, 10875,   420, 11900, 16080,\n",
       "            904,  9783,   355, 22464,  9658,   355,  8417,  2300, 13561,  2054,\n",
       "           4983,  4829, 30800,  7262,   420, 11394,  8417, 35143, 11937, 15682,\n",
       "           8417,  2300,  7283, 10875,   355,  1016,  4179,  5039, 14027, 26215,\n",
       "          26835,   671, 15095,   189,  1165, 15095, 11900,  6184,  1125,  3244,\n",
       "           3687,   622,  8785,  1121,   891, 13765,   671, 10199,   189,    13,\n",
       "            210,  6940,  3728,  9552,  6082,  8417,  2300,   916,  4375,   714,\n",
       "           3679,  1806, 10567,   915,   189,    13,   210, 16131, 11835,  8417,\n",
       "           2300,   189,    13, 24075,  3728,  8417,  2300,   916,  4829,  3687,\n",
       "           9000, 27689,  8417,  2300,  1300, 11243,  2062, 28431, 27689, 11835,\n",
       "           8417,  2300, 13224,  4829,  3687, 15964,   915,   189,    13, 27340,\n",
       "          11835,  8417,  2300,   189,    13, 27340,  3982,  3728,  8417,  2300,\n",
       "            916,  4375,  2450,  3272,  1234, 19083,   553,  3512,  3121,  1728,\n",
       "            641,  3092,  2113,  7843,   915,   189,    13,   210, 15402,  8417,\n",
       "           2300,   189,    13, 10057,   108, 12693, 14624, 29379,  4719,  6533,\n",
       "            739,   916, 16148, 10981, 21350,  9067,  1203,  8931,  1258, 11835,\n",
       "           4719,  6533,   739, 20393,   189,    13,   210, 19546,  1517, 11835,\n",
       "           8417,  2300,   189,    13,   210, 23928,   168,   117,   245,  6279,\n",
       "            114,   240,   170,   100,   124,   168,   117,   228,  6279,   100,\n",
       "            124,   168,   117,   228,   171,   238,   224, 41356,   236, 24175,\n",
       "          11082, 10981, 21350,  9067]])})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(enumerate(dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('<pad>', 3)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.pad_token, tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('</s>', 2)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.eos_token, tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step4 创建模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\"Langboat/bloom-389m-zh\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step5 配置训练参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = TrainingArguments(\n",
    "    output_dir=\"./causal_lm\",\n",
    "    per_device_train_batch_size=2,\n",
    "    gradient_accumulation_steps=16,\n",
    "    logging_steps=10,\n",
    "    num_train_epochs=1,\n",
    "    fp16=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step6 创建训练器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_18404\\1098143094.py:1: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    args=args,\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=tokenized_ds,\n",
    "    data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step7 模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='312' max='312' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [312/312 10:42, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>3.975000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>3.962700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>3.965300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>3.875800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>3.882800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>3.835700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>3.822800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>4.080500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>4.406900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>5.833000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>7.557200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>9.541500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>9.508700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>8.858200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>8.321800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>5.165200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>4.980800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>4.483700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>4.287400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>4.454500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>5.385900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>6.441100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>8.105500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>9.868000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>13.749300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>16.301600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>270</td>\n",
       "      <td>16.269000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>15.784900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>290</td>\n",
       "      <td>15.470100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>14.595500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>310</td>\n",
       "      <td>14.312900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=312, training_loss=7.947202181204771, metrics={'train_runtime': 643.6777, 'train_samples_per_second': 15.536, 'train_steps_per_second': 0.485, 'total_flos': 6092709626314752.0, 'train_loss': 7.947202181204771, 'epoch': 0.9984})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step8 模型推理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, device=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': '西安交通大学博物馆（Xi\\'an Jiaotong University Museum）是一座位于西安 -/：“（，,((（（（（--\"\\n。。----/)\\n\".，）「大， //。“。为, ******. （333'}]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe(\"西安交通大学博物馆（Xi'an Jiaotong University Museum）是一座位于西安\", max_length=128, do_sample=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'generated_text': '下面是一则游戏新闻。小编报道，近日，游戏产业发展的非常112、 (P））））与****45””），, -,；622222）是-**'}]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe(\"下面是一则游戏新闻。小编报道，近日，游戏产业发展的非常\", max_length=128, do_sample=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
